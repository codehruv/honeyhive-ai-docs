---
title: "Quickstart Tutorial"
description: "Get started by logging your LLM completion requests and user feedback with HoneyHive."
---


### Get API key

After signing up on the app, you can find your API key in the [Settings](https://app.honeyhive.ai/settings/account) page under Account.

### Install the SDK

<CodeGroup>
```bash Python
pip install honeyhive
```
</CodeGroup>

### Log your completion requests

This method allows you to log any arbitrary LLM requests on the client-side **without proxying your requests via HoneyHive servers**. Using this method, evaluation metrics such as custom metrics and AI feedback functions will be automatically computed based on the metrics you've defined and enabled in the [Metrics](https://app.honeyhive.ai/metrics) page. Learn more about defining evaluation metrics [here](/evaluation).

Let's start by running an OpenAI Chat Completion request and calculate a basic metric like latency.

<Note>We're using `OpenAI` models in this guide to simply demonstrate how to log requests with HoneyHive. You can alternatively use the SDK to log any arbitrary model completion requests across other model providers such as `Anthropic`, `Cohere`, `AI21 Labs`, or your own custom, self-hosted models.</Note>

<CodeGroup>
```python Python
import honeyhive
from honeyhive.sdk.utils import fill_template
import openai
import time

honeyhive.api_key = "HONEYHIVE_API_KEY"
openai.api_key = "OPENAI_API_KEY"

USER_TEMPLATE = "Write me an email on {{topic}} in a {{tone}} tone."
user_input = {
    "topic": "AI Services",
    "tone": "Friendly"
}
#"Write an email on AI Services in a Friendly tone."
user_message = fill_template(USER_TEMPLATE, user_input)

start = time.perf_counter()

openai_response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=100,
    top_p=1.0,
    messages=[
      {"role": "system", "content": "You are a helpful assistant who writes emails."},
      {"role": "user", "content": user_message}
    ]
)

end = time.perf_counter()

latency = (end - start)*1000
token_usage = openai_response.usage
```
</CodeGroup>

Now that you've run the request, let's try logging the request and some user metadata in HoneyHive.

<CodeGroup>
```python Python
response = honeyhive.generations.log(
    project="Sandbox - Email Writer",
    source="staging",
    model="gpt-3.5-turbo",
    generation=openai_response.choices[0].message.content,
    hyperparameters={
        "temperature": 0.7,
        "max_tokens": 100,
        "top_p": 1.0
    },
    prompt_template=USER_TEMPLATE,
    inputs=user_inputs,
    usage=token_usage,
    latency=request_latency,
    user_properties={
        "user_device": "Macbook Pro",
        "user_Id": "92739527492",
        "user_country": "United States",
        "user_subscriptiontier": "Enterprise",
        "user_tenantID": "Acme Inc."
    }
)
```
</CodeGroup>
<br/>
<Note>Using this method, you will not be able to use our Prompt CI/CD capabilities within the platform or calculate cost and latency metrics automatically. In order to update prompts, you will need to manually update the prompt, model provider and hyperparamater settings in your codebase when deploying new variants to production.</Note>

### Log user feedback and ground truth labels

Now that you've logged a request in HoneyHive, let's try logging user feedback and ground truth labels associated with that completion. 

Using the `generation_id` that is returned, you can send arbitrary feedback to HoneyHive using the `feedback` endpoint.

<CodeGroup>
```python Python
honeyhive.feedback(
    project="Sandbox - Email Writer",
    generation_id=response.generation_id,
    ground_truth="INSERT_GROUND_TRUTH_LABEL",
    feedback_json={
        "provided": True,
        "accepted": False,
        "edited": True
    }
)
```
</CodeGroup>

### [Optional] Proxy requests via HoneyHive

Alternatively, you can automatically call the current deployed prompt-model configuration within a specified project without specifying all the parameters. Using this method, we automatically route your requests to the model-prompt configuration that is currently active within the platform and capture some basic metrics like cost and latency.

More documentation can be found on our [saved prompt generations API page](/api-reference/generations/post_saved).

<CodeGroup>
```python Python
import honeyhive

honeyhive.api_key = "HONEYHIVE_API_KEY"
honeyhive.openai_api_key = "OPENAI_API_KEY"

response = honeyhive.generations.generate(
    project="Sandbox - Email Writer",
    source="staging",
    input={
        "topic": "Model evaluation for companies using GPT-4",
        "tone": "friendly"
    },
)
```
</CodeGroup>
