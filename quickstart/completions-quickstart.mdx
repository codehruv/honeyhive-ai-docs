---
title: "Logging LLM Requests"
description: "Get started by logging your LLM completion requests and user feedback with HoneyHive."
---


### Get API key

After signing up on the app, you can find your API key in the [Settings](https://app.honeyhive.ai/settings/account) page under Account.

### Install the SDK

We currently support a native Python SDK. For other languages, we encourage using HTTP request libraries to send requests.

<CodeGroup>
```bash Python
pip install honeyhive
```
</CodeGroup>

### Log your completion requests

This method allows you to log any arbitrary LLM requests on the client-side **without proxying your requests via HoneyHive servers**. Using this method, evaluation metrics such as custom metrics and AI feedback functions will be automatically computed based on the metrics you've defined and enabled in the [Metrics](https://app.honeyhive.ai/metrics) page. Learn more about defining evaluation metrics [here](/evaluation).

Let's start by running an OpenAI Chat Completion request and calculate a basic metric like latency.

<Note>We're using `OpenAI`, `Anthropic` and `Hugging Face` models in this guide to simply demonstrate how to log requests with HoneyHive. You can alternatively use the SDK to log any arbitrary model completion requests across other model providers such as `Cohere`, `AI21 Labs`, or your own custom, self-hosted models.</Note>

<CodeGroup>
```python OpenAI
import honeyhive
from honeyhive.sdk.utils import fill_template
import openai
import time

honeyhive.api_key = "HONEYHIVE_API_KEY"
openai.api_key = "OPENAI_API_KEY"

USER_TEMPLATE = "Write me an email on {{topic}} in a {{tone}} tone."
user_inputs = {
    "topic": "AI Services",
    "tone": "Friendly"
}
#"Write an email on AI Services in a Friendly tone."
user_message = fill_template(USER_TEMPLATE, user_inputs)

start = time.perf_counter()

openai_response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=100,
    messages=[
      {"role": "system", "content": "You are a helpful assistant who writes emails."},
      {"role": "user", "content": user_message}
    ]
)

end = time.perf_counter()

request_latency = (end - start)*1000
generation = openai_response.choices[0].message.content
token_usage = openai_response.usage
```
```python Anthropic
import honeyhive
from honeyhive.sdk.utils import fill_template
from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT
import time

honeyhive.api_key = "HONEYHIVE_API_KEY"

anthropic = Anthropic(
    api_key="ANTHROPIC_API_KEY",
)

USER_TEMPLATE = f"{HUMAN_PROMPT} Write me an email on {{topic}} in a {{tone}} tone.{AI_PROMPT}"
user_inputs = {
    "topic": "AI Services",
    "tone": "Friendly"
}
#"Write an email on AI Services in a Friendly tone."
user_message = fill_template(USER_TEMPLATE, user_inputs)

start = time.perf_counter()

completion = anthropic.completions.create(
    model="claude-2",
    max_tokens_to_sample=300,
    prompt=user_message
)

end = time.perf_counter()

request_latency = (end - start)*1000
generation = completion.completion
token_usage = {
    "completion_tokens": anthropic.count_tokens(completion.completion),
    "prompt_tokens": anthropic.count_tokens(user_message),
    "total_tokens": anthropic.count_tokens(completion.completion) + anthropic.count_tokens(user_message)
}
```
```python Open Source
import honeyhive
from honeyhive.sdk.utils import fill_template
from transformers import AutoTokenizer # & appropriate model imports
import time

honeyhive.api_key = "HONEYHIVE_API_KEY"

hf_model_path = "dummy/model"

tokenizer = AutoTokenizer.from_pretrained(hf_model_path)
# using the huggingface model import
# model = 

USER_TEMPLATE = f"{HUMAN_PROMPT} Write me an email on {{topic}} in a {{tone}} tone.{AI_PROMPT}"
user_inputs = {
    "topic": "AI Services",
    "tone": "Friendly"
}
#"Write an email on AI Services in a Friendly tone."
user_message = fill_template(USER_TEMPLATE, user_inputs)

start = time.perf_counter()

# tokenize the inputs
# model_inputs = tokenizer(user_message)

# generate model completion
# generated_ids = model.generate(**model_inputs)

# decode the tokens
# decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

end = time.perf_counter()

request_latency = (end - start)*1000
generation = decoded_output
token_usage = {
    "completion_tokens": len(generated_ids),
    "prompt_tokens": len(model_inputs),
    "total_tokens": len(generated_ids) + len(model_inputs)
}
```
</CodeGroup>

Now that you've run the request, let's try logging the request and some user metadata in HoneyHive. 

<Note>Adding a `session_id` field on the `metadata` field will enable session tracking on completions</Note>

<CodeGroup>
```python OpenAI
response = honeyhive.generations.log(
    project="Sandbox - Email Writer",
    source="staging",
    model="gpt-3.5-turbo",
    hyperparameters={
        "temperature": 0.7,
        "max_tokens": 100,
    },
    prompt_template=USER_TEMPLATE,
    inputs=user_inputs,
    generation=generation,
    metadata={
        "session_id": session_id  # Optionally specify a session id to track related completions
    },
    usage=token_usage,
    latency=request_latency,
    user_properties={
        "user_device": "Macbook Pro",
        "user_Id": "92739527492",
        "user_country": "United States",
        "user_subscriptiontier": "Enterprise",
        "user_tenantID": "Acme Inc."
    }
)
```
```python Anthropic
response = honeyhive.generations.log(
    project="Sandbox - Email Writer",
    source="staging",
    model="claude-2",
    hyperparameters={
        "max_tokens_to_sample": 300,
    },
    prompt_template=USER_TEMPLATE,
    inputs=user_inputs,
    generation=generation,
    metadata={
        "session_id": session_id  # Optionally specify a session id to track related completions
    },
    usage=token_usage,
    latency=request_latency,
    user_properties={
        "user_device": "Macbook Pro",
        "user_Id": "92739527492",
        "user_country": "United States",
        "user_subscriptiontier": "Enterprise",
        "user_tenantID": "Acme Inc."
    }
)
```
```python Open Source

gpu_cost_per_ms = 0.0000006
response = honeyhive.generations.log(
    project="Sandbox - Email Writer",
    source="staging",
    model=model_path,
    hyperparameters={
        # any additional arguments used when generating model completion
    },
    prompt_template=USER_TEMPLATE,
    inputs=user_inputs,
    generation=generation,
    metadata={
        "session_id": session_id  # Optionally specify a session id to track related completions
    },
    usage=token_usage,
    latency=request_latency,
    cost=request_latency * gpu_cost_per_ms,
    user_properties={
        "user_device": "Macbook Pro",
        "user_Id": "92739527492",
        "user_country": "United States",
        "user_subscriptiontier": "Enterprise",
        "user_tenantID": "Acme Inc."
    }
)
```
</CodeGroup>
<br/>
<Note>Using this method, you will not be able to use our Prompt CI/CD capabilities within the platform or calculate cost and latency metrics automatically. In order to update prompts, you will need to manually update the prompt, model provider and hyperparamater settings in your codebase when deploying new variants to production.</Note>

### Log user feedback and ground truth labels

Now that you've logged a request in HoneyHive, let's try logging user feedback and ground truth labels associated with that completion. 

Using the `generation_id` that is returned, you can send arbitrary feedback to HoneyHive using the `feedback` endpoint.

<CodeGroup>
```python Python
honeyhive.feedback(
    project="Sandbox - Email Writer",
    generation_id=response.generation_id,
    ground_truth="INSERT_GROUND_TRUTH_LABEL",
    feedback_json={
        "provided": True,
        "accepted": False,
        "edited": True
    }
)
```
</CodeGroup>

### [Optional] Proxy requests via HoneyHive

Alternatively, you can automatically call the current deployed prompt-model configuration within a specified project without specifying all the parameters. Using this method, we automatically route your requests to the model-prompt configuration that is currently active within the platform and capture some basic metrics like cost and latency.

More documentation can be found on our [saved prompt generations API page](/api-reference/generations/post_saved).

<CodeGroup>
```python Python
import honeyhive

honeyhive.api_key = "HONEYHIVE_API_KEY"
honeyhive.openai_api_key = "OPENAI_API_KEY"

response = honeyhive.generations.generate(
    project="Sandbox - Email Writer",
    source="staging",
    input={
        "topic": "Model evaluation for companies using GPT-4",
        "tone": "friendly"
    },
)
```
</CodeGroup>
