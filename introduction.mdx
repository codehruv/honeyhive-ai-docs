---
title: "Introducing HoneyHive"
description: "The developer platform for Generative AI"
---

### What is HoneyHive?

HoneyHive is the **developer platform** that helps you debug, evaluate and continuously improve your LLM apps with human feedback, quantitative rigor and safety best-practices. HoneyHive can help you **manage and version prompts, debug chains or agents, evaluate app variants against custom metrics and monitor performance metrics in production** - helping you iteratively take your LLM apps from prototype to production, and continuously improve them with data-driven insights.

![monitoring](/images/monitoring.png)

### Iterate and evaluate

Using HoneyHive, your team can continuously iterate on new prompts and models in the **Playground** with your entire team and evaluate any new app versions against a wide variety of **custom metrics** and **AI feedback functions** before pushing changes to production. This helps you debug errors, safely validate app performance and run back-tests to understand if new versions are potentially regressing.

### Monitor in production

Once in production, HoneyHive helps you discover new insights, understand user behavior and detect anomalies across your application pipeline. Get started by instrumenting your application to **log completion requests, user sessions or traces, user feedback, custom metrics and user-specific metadata**. You can quickly visualize any custom metrics, segment or compare data slices, and discover interesting clusters in your production data with embeddings maps.

Your team can use these insights to improve prompts with `Prompt Magic`, back-test new versions against your baseline variant and run live A/B tests in production to further validate performance improvements against user feedback or any custom metrics.

### Fine-tune custom models

To customize and optimize your models, you can use your production logs to quickly fine-tune OpenAI models or export datasets to fine-tune your own custom, open-source model with 3rd-party products. Once you have fine-tuned a model, you can quickly run a quantitative evaluation against your baseline variant to validate performance improvements before deploying the new model to production.

### Integration

Our SDK is designed to be developer-friendly and integrate with your existing infrastructure and the larger LLM ecosystem (Langchain, LlamaIndex, Pinecone, Chroma, etc.). 

<Tip>All features within the platform are programmatically accessible via the SDK.</Tip>

<br />

<CardGroup>
  <Card title="Log Requests & Feedback" icon="rectangle-terminal" href="/python-quickstart">
    How to quickly set up logging with HoneyHive.
  </Card>
  <Card title="Log Evaluation Runs" icon="rectangle-terminal" href="/programmatic-evals">
    How to run pipeline evaluations programmatically via the SDK.
  </Card>
  <Card title="Create Evaluation Metrics" icon="rectangle-terminal" href="/metrics">
    How to set up metrics and run evaluations in HoneyHive.
  </Card>
  <Card title="Concepts" icon="sliders" href="/concepts">
    Some key concepts behind our platform.
  </Card>
  <Card title="API Reference Guide" icon="heading" href="/api-reference/authentication">
    Our reference guide on how to integrate the HoneyHive SDK and APIs with your application.
  </Card>
  <Card title="Prompt Engineering and Fine-Tuning Guides" icon="heading" href="/resources">
    Guides for prompt engineering and fine-tuning your models.
  </Card>
</CardGroup>

<br />

<Note>Our documentation is a work in progress. If you have any questions, please reach out to us at [dhruv@honeyhive.ai](mailto:dhruv@honeyhive.ai).</Note>
